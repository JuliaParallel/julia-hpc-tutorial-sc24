{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia GPU packages are easy to install: Just do a `Pkg.add(\"CUDA\")`. The only thing you need, is a functional NVIDIA driver, but you don't need to install the CUDA toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train920/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train920/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train920/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train920/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; \n",
    "Pkg.add(\"BenchmarkTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA.jl check if the package is functional, you can call the `versioninfo()` function. Like `Base.versioninfo()`, this will print some information on the available hardware and loaded libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.2, local installation\n",
      "CUDA driver 12.6\n",
      "NVIDIA driver 535.183.6, originally for CUDA 12.2\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.2.1\n",
      "- CURAND: 10.3.3\n",
      "- CUFFT: 11.0.8\n",
      "- CUSOLVER: 11.5.0\n",
      "- CUSPARSE: 12.1.1\n",
      "- CUPTI: 2023.2.0 (API 20.0.0)\n",
      "- NVML: 12.0.0+535.183.6\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.4.3\n",
      "- CUDA_Driver_jll: 0.9.2+0\n",
      "- CUDA_Runtime_jll: 0.14.1+0\n",
      "- CUDA_Runtime_Discovery: 0.3.5\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.9.4\n",
      "- LLVM: 14.0.6\n",
      "\n",
      "Preferences:\n",
      "- CUDA_Runtime_jll.version: 12.2\n",
      "- CUDA_Runtime_jll.local: true\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Two Vectors Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have as an example vectors add. Let assume you have two vectors $\\vec{a}$ and $\\vec{b}$ and you want to add them. You can do it in in many ways in Julia: \n",
    "1. simple for loop in CPU\n",
    "2. julia add (+) in CPU or in GPU\n",
    "3. GPU kernel programming in CUDA or kernel abstracton using CUDA as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let define our input a, b vectors, and output c vector in CPU\n",
    "vector_size = 1024\n",
    "a = rand(1:4, vector_size)\n",
    "b = rand(1:4, vector_size)\n",
    "c = zeros(Int, vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple loop CPU loop to add two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " ⋮\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 7\n",
       " 3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vadd(a, b, c)\n",
    "    for i in 1:vector_size\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "    return\n",
    "end\n",
    "vadd(a, b, c)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia add (+) operation in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " ⋮\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 7\n",
       " 3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! \n",
    "Let see how to use add (+) in GPU to add two vectors using GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need first to move a and b vectors to GPU and define new dc empty vector in GPU\n",
    "da = CuArray(a)\n",
    "db = CuArray(b)\n",
    "dc = CUDA.zeros(Int, size(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add `da` vector to `db` vector using `+` operator. Thanks to Julio multiple dispatch feature!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " ⋮\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 7\n",
       " 3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = da + db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now learn how to write gpu kernel with `CUDA.jl` in Julia.\n",
    "\n",
    "In array operations, `CUDA.jl`` can leverage implicit parallelism to automatically execute these operations in parallel on a GPU. However, when using kernels, it is the programmer's responsibility to effectively utilize the available parallel execution resources for the specific operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vadd(c, a, b)\n",
    "    # obtain thread index which should be map the index of a and b\n",
    "    i = threadIdx().x\n",
    "    # Each thread will add its own element to c\n",
    "    c[i] = a[i] + b[i]\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, that's pretty easy, you just need to write a scalar function and launch that function in parallel using the `@cuda` macro and its `threads` keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " ⋮\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 7\n",
       " 3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=length(a) vadd(dc, da, db)\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok this is great but try to set `vector_size` to 10240. You will notice that CPU simple loop and add (+) operator in the CPU and GPU are working, but your hand written GPU code is not working.\n",
    "\n",
    "Ouch what is going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs have a limited number of threads they can run on a single streaming multiprocessor (SM), but they also have multiple SMs.\n",
    "\n",
    "To take advantage of them all, we need to run a kernel with multiple blocks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA.jl, the expression `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` calculates a unique index for each thread across multiple blocks in a CUDA kernel execution. Here's a breakdown of each component and how they contribute to computing this index:\n",
    "\n",
    "- `threadIdx().x`: This returns the x-coordinate of the thread within its block. It's the thread's index within the block, starting from 1 (unlike C/C++ CUDA where it starts from 0).\n",
    "\n",
    "- `blockIdx().x`: This gives the x-coordinate of the block within the grid. It represents the block's index in the grid, also starting from 1.\n",
    "\n",
    "- `blockDim().x`: This represents the number of threads per block along the x-axis.\n",
    "\n",
    "The formula `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` is used to compute a global index for each thread. It positions the threads linearly across all blocks. Here's what each part does:\n",
    "\n",
    "- `(blockIdx().x - 1) * blockDim().x`: This part calculates the offset to the start of the current block. Subtracting 1 from `blockIdx().x` makes it zero-based, and then it is multiplied by the number of threads in each block `(blockDim().x)`. This gives the index of the first thread in the current block relative to the entire grid.\n",
    "\n",
    "- `threadIdx().x`: Adding this to the block offset gives the specific thread's index within the whole grid.\n",
    "\n",
    "It similer if you are working in 2D grids. The formula for 2D grids is `i = threadIdx().y * blockDim().y + threadIdx().y`. Here's what each part does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To know number of Threads per block\n",
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vadd(c, a, b)\n",
    "    # calculates a unique index for each thread across multiple blocks\n",
    "    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    if i <= length(a)\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " ⋮\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 7\n",
       " 3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=1024 blocks=cld(length(da),1024) vadd(dc, da, db)\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Matrix Multiplication Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_size = 2048\n",
    "A = rand(matrix_size, matrix_size)\n",
    "B = rand(matrix_size, matrix_size)\n",
    "C = zeros(matrix_size, matrix_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three nested loops implmentation of matrix multiplication in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixMultiplication! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function MatrixMultiplication!(A, B, C)\n",
    "    for i in 1:matrix_size\n",
    "        Threads.@threads for j in 1:matrix_size\n",
    "            C[i, j] = 0\n",
    "            for k in 1:matrix_size\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia mutiplication (*) operation in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 Matrix{Float64}:\n",
       " 497.727  498.172  505.842  496.282  …  518.785  515.704  511.647  527.195\n",
       " 512.367  512.068  517.075  508.459     528.052  527.279  525.13   529.496\n",
       " 508.418  513.269  519.497  504.399     529.693  520.459  516.247  525.33\n",
       " 507.398  524.236  517.82   512.141     524.056  533.715  523.636  527.291\n",
       " 518.574  525.062  519.645  523.654     541.132  533.416  529.764  534.653\n",
       " 502.198  519.423  513.635  504.205  …  521.792  529.731  523.782  529.043\n",
       " 507.997  516.699  509.864  512.743     538.144  528.51   524.629  521.334\n",
       " 519.55   519.476  522.8    508.789     534.098  536.234  529.813  529.768\n",
       " 488.082  501.328  503.469  493.955     511.72   514.566  506.7    512.4\n",
       " 501.65   512.958  513.608  501.925     520.92   531.522  518.559  514.894\n",
       " 500.202  513.806  508.432  504.929  …  518.806  519.545  516.367  513.643\n",
       " 514.988  517.088  520.564  518.217     535.746  531.564  525.136  530.639\n",
       " 499.068  513.717  514.405  510.414     531.018  529.449  520.731  523.751\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 508.219  514.886  525.206  525.279     532.27   530.46   522.179  526.584\n",
       " 504.659  514.513  513.295  500.321     523.856  521.226  522.885  524.046\n",
       " 515.93   512.808  520.438  514.059     527.277  535.639  520.06   521.474\n",
       " 504.999  516.929  528.887  515.714     534.226  524.522  534.063  532.467\n",
       " 505.697  523.915  516.149  515.318  …  530.353  534.54   521.058  525.265\n",
       " 511.822  524.797  523.762  519.011     539.7    532.05   530.278  528.651\n",
       " 500.909  515.474  518.085  511.063     525.995  528.548  525.189  530.628\n",
       " 502.992  518.247  523.003  511.181     530.582  529.505  522.229  526.834\n",
       " 493.537  512.192  512.293  506.184     524.624  523.354  514.072  523.314\n",
       " 510.839  521.333  523.554  518.527  …  539.883  528.722  528.302  536.928\n",
       " 502.43   510.042  508.163  502.86      531.717  528.79   519.478  519.069\n",
       " 487.813  505.746  508.779  495.619     522.31   511.31   508.116  518.411"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 45 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 78.016 ms\u001b[22m\u001b[39m … \u001b[35m217.613 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m104.030 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m107.660 ms\u001b[22m\u001b[39m ± \u001b[32m 24.252 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m1.24% ± 2.68%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m▆\u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m▁\u001b[34m█\u001b[39m\u001b[39m▁\u001b[32m \u001b[39m\u001b[39m▆\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▄\u001b[39m█\u001b[39m▄\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m▄\u001b[39m\u001b[39m█\u001b[39m▄\u001b[39m█\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▁\n",
       "  78 ms\u001b[90m            Histogram: frequency by time\u001b[39m          218 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m32.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m2\u001b[39m."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark  A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let see how to use add (*) in GPU to mutiply two matrices using GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float32, 2, CUDA.DeviceMemory}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need first to move A and B matrces to GPU and define new DC empty matrix in GPU\n",
    "DA = CuArray(A)\n",
    "DB = CuArray(B)\n",
    "DC = CUDA.zeros(size(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same way here we can multiply `DA` matrix by `DB` matrix using `*` operator. Thanks again to Julio multiple dispatch feature!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 497.727  498.172  505.842  496.282  …  518.785  515.704  511.647  527.195\n",
       " 512.367  512.068  517.075  508.459     528.052  527.279  525.13   529.496\n",
       " 508.418  513.269  519.497  504.399     529.693  520.459  516.247  525.33\n",
       " 507.398  524.236  517.82   512.141     524.056  533.715  523.636  527.291\n",
       " 518.574  525.062  519.645  523.654     541.132  533.416  529.764  534.653\n",
       " 502.198  519.423  513.635  504.205  …  521.792  529.731  523.782  529.043\n",
       " 507.997  516.699  509.864  512.743     538.144  528.51   524.629  521.334\n",
       " 519.55   519.476  522.8    508.789     534.098  536.234  529.813  529.768\n",
       " 488.082  501.328  503.469  493.955     511.72   514.566  506.7    512.4\n",
       " 501.65   512.958  513.608  501.925     520.92   531.522  518.559  514.894\n",
       " 500.202  513.806  508.432  504.929  …  518.806  519.545  516.367  513.643\n",
       " 514.988  517.088  520.564  518.217     535.746  531.564  525.136  530.639\n",
       " 499.068  513.717  514.405  510.414     531.018  529.449  520.731  523.751\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 508.219  514.886  525.206  525.279     532.27   530.46   522.179  526.584\n",
       " 504.659  514.513  513.295  500.321     523.856  521.226  522.885  524.046\n",
       " 515.93   512.808  520.438  514.059     527.277  535.639  520.06   521.474\n",
       " 504.999  516.929  528.887  515.714     534.226  524.522  534.063  532.467\n",
       " 505.697  523.915  516.149  515.318  …  530.353  534.54   521.058  525.265\n",
       " 511.822  524.797  523.762  519.011     539.7    532.05   530.278  528.651\n",
       " 500.909  515.474  518.085  511.063     525.995  528.548  525.189  530.628\n",
       " 502.992  518.247  523.003  511.181     530.582  529.505  522.229  526.834\n",
       " 493.537  512.192  512.293  506.184     524.624  523.354  514.072  523.314\n",
       " 510.839  521.333  523.554  518.527  …  539.883  528.722  528.302  536.928\n",
       " 502.43   510.042  508.163  502.86      531.717  528.79   519.478  519.069\n",
       " 487.813  505.746  508.779  495.619     522.31   511.31   508.116  518.411"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DC = DA * DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 5883 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 17.414 μs\u001b[22m\u001b[39m … \u001b[35m  6.583 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 80.61%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m972.299 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m834.409 μs\u001b[22m\u001b[39m ± \u001b[32m357.439 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.36% ±  2.17%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m█\u001b[34m \u001b[39m\u001b[39m▂\n",
       "  17.4 μs\u001b[90m          Histogram: frequency by time\u001b[39m          976 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.12 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m47\u001b[39m."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark  DA * DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixMultiplication! (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function MatrixMultiplication!(A,B,C)\n",
    "\n",
    "    row = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    col = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "\n",
    "    sum = zero(eltype(C))\n",
    "\n",
    "    if row <= size(A, 1) && col < size(B, 2)\n",
    "        for i = 1:size(A, 2)\n",
    "\n",
    "            #@inbounds disables bounds checking for array accesses for performance optimization.\n",
    "            @inbounds sum += A[row, i] * B[i, col]\n",
    "        end\n",
    "        C[row, col] = sum\n",
    "    end\n",
    "\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 497.727  498.172  505.842  496.282  …  518.785  515.704  511.647  527.195\n",
       " 512.367  512.068  517.075  508.459     528.052  527.279  525.13   529.496\n",
       " 508.418  513.269  519.497  504.399     529.693  520.459  516.247  525.33\n",
       " 507.398  524.236  517.82   512.141     524.056  533.715  523.636  527.291\n",
       " 518.574  525.062  519.645  523.654     541.132  533.416  529.764  534.653\n",
       " 502.198  519.423  513.635  504.205  …  521.792  529.731  523.782  529.043\n",
       " 507.997  516.699  509.864  512.743     538.144  528.51   524.629  521.334\n",
       " 519.55   519.476  522.8    508.789     534.098  536.234  529.813  529.768\n",
       " 488.082  501.328  503.469  493.955     511.72   514.566  506.7    512.4\n",
       " 501.65   512.958  513.608  501.925     520.92   531.522  518.559  514.894\n",
       " 500.202  513.806  508.432  504.929  …  518.806  519.545  516.367  513.643\n",
       " 514.988  517.088  520.564  518.217     535.746  531.564  525.136  530.639\n",
       " 499.068  513.717  514.405  510.414     531.018  529.449  520.731  523.751\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 508.219  514.886  525.206  525.279     532.27   530.46   522.179  526.584\n",
       " 504.659  514.513  513.295  500.321     523.856  521.226  522.885  524.046\n",
       " 515.93   512.808  520.438  514.059     527.277  535.639  520.06   521.474\n",
       " 504.999  516.929  528.887  515.714     534.226  524.522  534.063  532.467\n",
       " 505.697  523.915  516.149  515.318  …  530.353  534.54   521.058  525.265\n",
       " 511.822  524.797  523.762  519.011     539.7    532.05   530.278  528.651\n",
       " 500.909  515.474  518.085  511.063     525.995  528.548  525.189  530.628\n",
       " 502.992  518.247  523.003  511.181     530.582  529.505  522.229  526.834\n",
       " 493.537  512.192  512.293  506.184     524.624  523.354  514.072  523.314\n",
       " 510.839  521.333  523.554  518.527  …  539.883  528.722  528.302  536.928\n",
       " 502.43   510.042  508.163  502.86      531.717  528.79   519.478  519.069\n",
       " 487.813  505.746  508.779  495.619     522.31   511.31   508.116  518.411"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication!(DA, DB, DC)\n",
    "DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 304 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m16.102 ms\u001b[22m\u001b[39m … \u001b[35m16.930 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m16.198 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m16.201 ms\u001b[22m\u001b[39m ± \u001b[32m63.703 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▆\u001b[39m \u001b[34m▆\u001b[39m\u001b[32m \u001b[39m\u001b[39m▂\u001b[39m▆\u001b[39m█\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[32m▇\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▇\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[39m▄\n",
       "  16.1 ms\u001b[90m         Histogram: frequency by time\u001b[39m        16.3 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.33 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m44\u001b[39m."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark CUDA.@sync @cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication!(DA, DB, DC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch!! why the kernel implmentation is slower than Julia multiplication operator?\n",
    "\n",
    "The answer is that this is only the naive implementation of matrix multiplication in Julia. The performant implementation relies on tiling, where the matrix is divided into smaller submatrices (tiles) that fit more effectively within the GPU’s memory hierarchy, including shared memory and cache. By processing these tiles independently, the GPU can optimize memory access patterns and minimize data transfer overhead.\n",
    "\n",
    "In a tiled implementation, each thread block on the GPU handles a specific tile of the output matrix, loading portions of the input tiles into shared memory to reduce the repeated global memory access. This approach enables a higher level of parallelism by allowing multiple tiles to be processed concurrently across the GPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explor the naive matrix multiplication example using Kernel abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train920/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train920/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; \n",
    "Pkg.add(\"KernelAbstractions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using KernelAbstractions\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note how to write a kernel in KernelAbstractions.jl. There are minimal changes compared to the vendor package. It efficiently abstracts away the index calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixMultiplication_kernel! (generic function with 4 methods)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function MatrixMultiplication_kernel!(A, B, C)\n",
    "    #Global index of  each thread across multiple blocks in both x and y dimension of the grid\n",
    "    row, col = @index(Global, NTuple)\n",
    "\n",
    "    sum = zero(eltype(C))\n",
    "\n",
    "    if row <= size(A, 1) && col <= size(B, 2)\n",
    "        for i = 1:size(A, 2)\n",
    "             @inbounds sum += A[row, i] * B[i, col]\n",
    "        end\n",
    "        @inbounds C[row, col] = sum\n",
    "     end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Backend =  CUDA.CUDABackend()\n",
    "matrix_size = 2048\n",
    "T = Float64\n",
    "DA = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "DB = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "DC = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n",
    "\n",
    "workgroupsize = (32, 32)\n",
    "\n",
    "kernel! = MatrixMultiplication_kernel!(Backend, workgroupsize)\n",
    "kernel!(DA, DB, DC, ndrange=(size(DC)))\n",
    "KernelAbstractions.synchronize(Backend)\n",
    "\n",
    "isapprox(DC, DA * DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 348 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m13.950 ms\u001b[22m\u001b[39m … \u001b[35m14.059 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m14.013 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m14.012 ms\u001b[22m\u001b[39m ± \u001b[32m20.283 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▅\u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▁\u001b[39m▂\u001b[39m▆\u001b[39m▅\u001b[39m▃\u001b[34m▄\u001b[39m\u001b[39m▂\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m \u001b[39m▅\n",
       "  14 ms\u001b[90m           Histogram: frequency by time\u001b[39m        14.1 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.89 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m60\u001b[39m."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark begin\n",
    "    kernel!(DA, DB, DC, ndrange=(size(DC)))\n",
    "    KernelAbstractions.synchronize(Backend)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory copy with KernelAbstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have another example to show how to use shared memory in KernelAbstractions. This kernel performs a matrix copy using local memory (also known as shared memory in CUDA), which can significantly speed up the memory access times by reducing global memory bandwidth usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lmem_copy_kernel! (generic function with 4 methods)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function lmem_copy_kernel!(output, @Const(input))\n",
    "\n",
    "    # Gets the global index of the thread in a multidimensional grid, which is used to index into the global input and output arrays.\n",
    "    I, J= @index(Global, NTuple) \n",
    "    # Gets the local index within a thread block or workgroup, useful for indexing into locally shared memory.\n",
    "    i, j = @index(Local, NTuple) # Local index of thread\n",
    "\n",
    "    #@groupsize() retrieves the dimensions of the thread block or workgroup. \n",
    "    #The @uniform ensures that these values are treated as constants that are the same for all threads.\n",
    "    N = @uniform @groupsize()[1] # blockDim.x \n",
    "    M = @uniform @groupsize()[2] # blockDim.y\n",
    "    \n",
    "    tile = @localmem eltype(output) (N, M) # Allocate local (shared) memory\n",
    "\n",
    "    #First, data from the global input array is loaded into the shared tile array using local indices.\n",
    "    @inbounds tile[i, j] = input[I, J]\n",
    "\n",
    "    #@synchronize ensures that all threads in the workgroup have completed their memory writes to the shared memory before proceeding. \n",
    "    #This is crucial to prevent race conditions.\n",
    "    @synchronize\n",
    "\n",
    "    #Finally, the data is written back from the shared tile array to the global output array.\n",
    "    @inbounds output[I, J] = tile[i, j]\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "output = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n",
    "\n",
    "const lmem_copy! = lmem_copy_kernel!(Backend, workgroupsize)\n",
    "lmem_copy!(output, input, ndrange=size(input))\n",
    "KernelAbstractions.synchronize(Backend)\n",
    "\n",
    "all(input == output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
