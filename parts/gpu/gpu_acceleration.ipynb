{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia GPU packages are easy to install: Just do a `Pkg.add(\"CUDA\")`. The only thing you need, is a functional NVIDIA driver, but you don't need to install the CUDA toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg\n",
    "#Pkg.add(\"CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Pkg; \n",
    "#Pkg.add(\"BenchmarkTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA.jl check if the package is functional, you can call the `versioninfo()` function. Like `Base.versioninfo()`, this will print some information on the available hardware and loaded libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 11.8, artifact installation\n",
      "CUDA driver 11.8\n",
      "NVIDIA driver 520.61.5\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 11.11.3\n",
      "- CURAND: 10.3.0\n",
      "- CUFFT: 10.9.0\n",
      "- CUSOLVER: 11.4.1\n",
      "- CUSPARSE: 11.7.5\n",
      "- CUPTI: 2022.3.0 (API 18.0.0)\n",
      "- NVML: 11.0.0+520.61.5\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.4.3\n",
      "- CUDA_Driver_jll: 0.9.1+1\n",
      "- CUDA_Runtime_jll: 0.14.1+0\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.11.1\n",
      "- LLVM: 16.0.6\n",
      "\n",
      "2 devices:\n",
      "  0: Tesla V100-PCIE-16GB (sm_70, 15.778 GiB / 16.000 GiB available)\n",
      "  1: Tesla V100-PCIE-16GB (sm_70, 15.778 GiB / 16.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Two Vectors Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have as an example vectors add. Let assume you have two vectors $\\vec{a}$ and $\\vec{b}$ and you want to add them. You can do it in in many ways in Julia: \n",
    "1. simple for loop in CPU\n",
    "2. julia add (+) in CPU or in GPU\n",
    "3. GPU kernel programming in CUDA or kernel abstracton using CUDA as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let define our input a, b vectors, and output c vector in CPU\n",
    "vector_size = 1024\n",
    "a = rand(1:4, vector_size)\n",
    "b = rand(1:4, vector_size)\n",
    "c = zeros(Int, vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple loop CPU loop to add two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in 1:10\n",
    "    c[i] = a[i] + b[i]\n",
    "end\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia add (+) operation in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " ⋮\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 6\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 6\n",
       " 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! \n",
    "Let see how to use add (+) in GPU to add two vectors using GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need first to move a and b vectors to GPU and define new dc empty vector in GPU\n",
    "da = CuArray(a)\n",
    "db = CuArray(b)\n",
    "dc = CUDA.zeros(Int, size(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add `da` vector to `db` vector using `+` operator. Thanks to Julio multiple dispatch feature!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " ⋮\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 6\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 6\n",
       " 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc = da + db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now learn how to write gpu kernel with `CUDA.jl` in Julia.\n",
    "\n",
    "In array operations, `CUDA.jl`` can leverage implicit parallelism to automatically execute these operations in parallel on a GPU. However, when using kernels, it is the programmer's responsibility to effectively utilize the available parallel execution resources for the specific operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vadd(c, a, b)\n",
    "    # obtain thread index which should be map the index of a and b\n",
    "    i = threadIdx().x\n",
    "    # Each thread will add its own element to c\n",
    "    c[i] = a[i] + b[i]\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, that's pretty easy, you just need to write a scalar function and launch that function in parallel using the `@cuda` macro and its `threads` keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " ⋮\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 6\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 6\n",
       " 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@cuda threads=length(a) vadd(dc, da, db)\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok this is great but try to set `vector_size` to 10240. You will notice that CPU simple loop and add (+) operator in the CPU and GPU are working, but your hand written GPU code is not working.\n",
    "\n",
    "Ouch what is going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs have a limited number of threads they can run on a single streaming multiprocessor (SM), but they also have multiple SMs.\n",
    "\n",
    "To take advantage of them all, we need to run a kernel with multiple blocks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA.jl, the expression `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` calculates a unique index for each thread across multiple blocks in a CUDA kernel execution. Here's a breakdown of each component and how they contribute to computing this index:\n",
    "\n",
    "- `threadIdx().x`: This returns the x-coordinate of the thread within its block. It's the thread's index within the block, starting from 1 (unlike C/C++ CUDA where it starts from 0).\n",
    "\n",
    "- `blockIdx().x`: This gives the x-coordinate of the block within the grid. It represents the block's index in the grid, also starting from 1.\n",
    "\n",
    "- `blockDim().x`: This represents the number of threads per block along the x-axis.\n",
    "\n",
    "The formula `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` is used to compute a global index for each thread. It positions the threads linearly across all blocks. Here's what each part does:\n",
    "\n",
    "- `(blockIdx().x - 1) * blockDim().x`: This part calculates the offset to the start of the current block. Subtracting 1 from `blockIdx().x` makes it zero-based, and then it is multiplied by the number of threads in each block `(blockDim().x)`. This gives the index of the first thread in the current block relative to the entire grid.\n",
    "\n",
    "- `threadIdx().x`: Adding this to the block offset gives the specific thread's index within the whole grid.\n",
    "\n",
    "It similer if you are working in 2D grids. The formula for 2D grids is `i = threadIdx().y * blockDim().y + threadIdx().y`. Here's what each part does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To know number of Threads per block\n",
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vadd(c, a, b)\n",
    "    # calculates a unique index for each thread across multiple blocks\n",
    "    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    if i <= length(a)\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 2\n",
       " 4\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 7\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " ⋮\n",
       " 5\n",
       " 3\n",
       " 8\n",
       " 4\n",
       " 5\n",
       " 5\n",
       " 5\n",
       " 7\n",
       " 8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@cuda threads=1024 blocks=cld(length(da),1024) vadd(dc, da, db)\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Matrix Multiplication Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix_size = 2048\n",
    "A = rand(matrix_size, matrix_size)\n",
    "B = rand(matrix_size, matrix_size)\n",
    "C = zeros(matrix_size, matrix_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three nested loops implmentation of matrix multiplication in CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:matrix_size\n",
    "    Threads.@threads for j in 1:matrix_size\n",
    "        C[i, j] = 0\n",
    "        for k in 1:matrix_size\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia mutiplication (*) operation in CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 Matrix{Float64}:\n",
       " 509.089  510.348  502.856  507.543  …  514.401  504.3    503.37   516.114\n",
       " 519.232  526.566  515.182  514.504     511.492  517.574  519.79   525.397\n",
       " 513.519  505.171  508.243  507.533     507.379  505.279  503.33   512.835\n",
       " 518.391  519.596  513.286  515.529     512.007  514.813  515.499  522.976\n",
       " 511.13   521.708  506.156  511.011     510.027  510.334  521.786  517.15\n",
       " 510.033  512.843  502.064  509.046  …  507.186  514.462  511.407  507.398\n",
       " 500.526  502.503  498.32   503.397     501.819  507.197  504.563  509.718\n",
       " 517.251  524.602  511.128  517.558     521.457  518.142  513.97   521.908\n",
       " 510.031  517.331  502.657  512.068     512.042  503.297  507.88   508.645\n",
       " 505.563  508.921  507.215  500.042     508.242  507.166  511.379  515.96\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 511.885  514.335  509.953  511.562     512.467  517.093  511.803  518.788\n",
       " 511.204  517.509  503.653  507.83   …  504.19   508.313  510.737  510.62\n",
       " 514.399  517.515  512.606  511.456     518.287  515.479  512.887  520.437\n",
       " 515.274  522.813  509.557  515.493     516.287  513.686  515.976  527.502\n",
       " 518.89   525.198  512.03   517.823     515.208  516.935  523.39   525.354\n",
       " 506.136  508.675  497.239  500.232     499.925  505.58   506.49   507.067\n",
       " 508.718  515.899  500.492  512.586  …  516.082  510.071  511.976  514.881\n",
       " 516.767  523.267  512.018  519.028     520.932  515.002  516.865  525.193\n",
       " 504.15   514.375  498.581  513.23      502.436  502.403  513.831  516.324"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 104 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m42.659 ms\u001b[22m\u001b[39m … \u001b[35m59.537 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 25.34%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m48.161 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m7.84%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m48.459 ms\u001b[22m\u001b[39m ± \u001b[32m 3.254 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m7.29% ±  5.01%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▅\u001b[39m█\u001b[39m▂\u001b[39m█\u001b[34m▅\u001b[39m\u001b[32m▆\u001b[39m\u001b[39m▅\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m█\u001b[39m▅\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▁\u001b[39m▇\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▄\n",
       "  42.7 ms\u001b[90m         Histogram: frequency by time\u001b[39m        59.1 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m32.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m3\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark  A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let see how to use add (*) in GPU to mutiply two matrices using GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float32, 2, CUDA.DeviceMemory}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need first to move A and B matrces to GPU and define new DC empty matrix in GPU\n",
    "DA = CuArray(A)\n",
    "DB = CuArray(B)\n",
    "DC = CUDA.zeros(size(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same way here we can multiply `DA` matrix by `DB` matrix using `*` operator. Thanks again to Julio multiple dispatch feature!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 510.872  507.187  520.179  518.381  …  507.109  510.753  528.952  515.195\n",
       " 515.141  509.509  518.769  519.114     501.728  512.96   531.92   509.071\n",
       " 508.376  505.47   517.744  508.491     508.649  503.993  529.625  516.51\n",
       " 509.12   505.797  512.938  509.629     497.167  500.447  518.173  507.102\n",
       " 516.88   505.166  517.104  517.06      505.078  515.646  532.486  526.457\n",
       " 518.367  504.533  518.646  511.652  …  505.951  507.462  525.726  514.54\n",
       " 517.696  512.686  523.199  513.128     502.774  508.019  523.233  516.018\n",
       " 528.937  523.459  526.988  523.026     516.537  524.125  535.253  524.216\n",
       " 525.609  521.547  521.866  522.177     515.77   514.085  534.858  520.169\n",
       " 516.009  508.626  515.507  513.631     497.013  510.418  526.231  512.995\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 512.155  499.859  509.823  509.83      500.542  504.753  516.202  511.068\n",
       " 526.594  505.914  522.545  516.125  …  511.52   519.952  523.347  517.316\n",
       " 514.319  498.914  514.747  510.812     496.43   503.257  516.002  505.019\n",
       " 511.204  502.843  516.426  511.09      506.726  505.012  518.966  506.73\n",
       " 516.515  512.39   525.044  519.272     507.733  514.769  529.238  517.259\n",
       " 510.492  506.286  517.512  507.121     500.586  505.722  518.382  510.834\n",
       " 505.975  503.348  512.383  503.198  …  494.252  498.996  520.178  513.582\n",
       " 529.262  514.444  520.826  521.525     508.395  519.821  531.88   521.204\n",
       " 510.15   503.838  508.404  514.754     505.913  507.698  523.216  511.64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DC = DA * DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 2113 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m22.177 μs\u001b[22m\u001b[39m … \u001b[35m  2.743 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m 2.728 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m 2.366 ms\u001b[22m\u001b[39m ± \u001b[32m921.696 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.08% ± 1.42%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[34m \u001b[39m\u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[34m \u001b[39m\u001b[39m▂\n",
       "  22.2 μs\u001b[90m         Histogram: frequency by time\u001b[39m         2.74 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.22 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m53\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark  DA * DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixMultiplication! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function MatrixMultiplication!(A,B,C)\n",
    "\n",
    "    row = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    col = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "\n",
    "    sum = zero(eltype(C))\n",
    "\n",
    "    if row <= size(A, 1) && col < size(B, 2)\n",
    "        for i = 1:size(A, 2)\n",
    "\n",
    "            #@inbounds disables bounds checking for array accesses for performance optimization.\n",
    "            @inbounds sum += A[row, i] * B[i, col]\n",
    "        end\n",
    "        C[row, col] = sum\n",
    "    end\n",
    "\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 510.872  507.187  520.179  518.381  …  507.109  510.753  528.952  515.195\n",
       " 515.141  509.509  518.769  519.114     501.728  512.96   531.92   509.071\n",
       " 508.376  505.47   517.744  508.491     508.649  503.993  529.625  516.51\n",
       " 509.12   505.797  512.938  509.629     497.167  500.447  518.173  507.102\n",
       " 516.88   505.166  517.104  517.06      505.078  515.646  532.486  526.457\n",
       " 518.367  504.533  518.646  511.652  …  505.951  507.462  525.726  514.54\n",
       " 517.696  512.686  523.199  513.128     502.774  508.019  523.233  516.018\n",
       " 528.937  523.459  526.988  523.026     516.537  524.125  535.253  524.216\n",
       " 525.609  521.547  521.866  522.177     515.77   514.085  534.858  520.169\n",
       " 516.009  508.626  515.507  513.631     497.013  510.418  526.231  512.995\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 512.155  499.859  509.823  509.83      500.542  504.753  516.202  511.068\n",
       " 526.594  505.914  522.545  516.125  …  511.52   519.952  523.347  517.316\n",
       " 514.319  498.914  514.747  510.812     496.43   503.257  516.002  505.019\n",
       " 511.204  502.843  516.426  511.09      506.726  505.012  518.966  506.73\n",
       " 516.515  512.39   525.044  519.272     507.733  514.769  529.238  517.259\n",
       " 510.492  506.286  517.512  507.121     500.586  505.722  518.382  510.834\n",
       " 505.975  503.348  512.383  503.198  …  494.252  498.996  520.178  513.582\n",
       " 529.262  514.444  520.826  521.525     508.395  519.821  531.88   521.204\n",
       " 510.15   503.838  508.404  514.754     505.913  507.698  523.216  511.64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication!(DA, DB, DC)\n",
    "DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 257 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m19.359 ms\u001b[22m\u001b[39m … \u001b[35m19.665 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m19.457 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m19.459 ms\u001b[22m\u001b[39m ± \u001b[32m29.755 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▅\u001b[39m▂\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▃\u001b[39m \u001b[39m▁\u001b[34m▇\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m█\u001b[39m▄\u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▃\u001b[39m▄\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m \u001b[39m▃\n",
       "  19.4 ms\u001b[90m         Histogram: frequency by time\u001b[39m        19.5 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.05 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m42\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark CUDA.@sync @cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication!(DA, DB, DC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch!! why the kernel implmentation is slower than Julia multiplication operator?\n",
    "\n",
    "The answer is that this is only the naive implementation of matrix multiplication in Julia. The performant implementation relies on tiling, where the matrix is divided into smaller submatrices (tiles) that fit more effectively within the GPU’s memory hierarchy, including shared memory and cache. By processing these tiles independently, the GPU can optimize memory access patterns and minimize data transfer overhead.\n",
    "\n",
    "In a tiled implementation, each thread block on the GPU handles a specific tile of the output matrix, loading portions of the input tiles into shared memory to reduce the repeated global memory access. This approach enables a higher level of parallelism by allowing multiple tiles to be processed concurrently across the GPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explor the naive matrix multiplication example using Kernel abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Pkg; \n",
    "#Pkg.add(\"KernelAbstractions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note how to write a kernel in KernelAbstractions.jl. There are minimal changes compared to the vendor package. It efficiently abstracts away the index calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel function MatrixMultiplication!(A, B, C)\n",
    "    #Global index of  each thread across multiple blocks in both x and y dimension of the grid\n",
    "    row, col = @index(Global, NTuple)\n",
    "\n",
    "    sum = zero(eltype(C))\n",
    "\n",
    "    if row <= size(A, 1) && col <= size(B, 2)\n",
    "\tfor i = 1:size(A, 2)\n",
    "\t   @inbounds sum += A[row, i] * B[i, col]\n",
    "\tend\n",
    "\t@inbounds C[row, col] = sum\n",
    "     end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Backend =  CUDA.CUDABackend()\n",
    "matrix_size = 2048\n",
    "T = Float64\n",
    "DA = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "DB = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "DC = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n",
    "\n",
    "workgroupsize = (32, 32)\n",
    "\n",
    "kernel! = MatrixMultiplication!(Backend, workgroupsize)\n",
    "kernel!(DA, DB, DC, ndrange=(size(DC)))\n",
    "KernelAbstractions.synchronize(Backend)\n",
    "\n",
    "isapprox(DC, DA * DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 270 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m18.389 ms\u001b[22m\u001b[39m … \u001b[35m18.726 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m18.517 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m18.502 ms\u001b[22m\u001b[39m ± \u001b[32m47.934 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[34m▂\u001b[39m\u001b[39m▁\u001b[39m \u001b[39m█\u001b[39m▃\u001b[39m▇\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▁\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▆\u001b[39m▆\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▃\u001b[39m█\u001b[32m▇\u001b[39m\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[39m▃\n",
       "  18.4 ms\u001b[90m         Histogram: frequency by time\u001b[39m        18.6 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.61 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m58\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark begin\n",
    "    kernel!(DA, DB, DC, ndrange=(size(DC)))\n",
    "    KernelAbstractions.synchronize(Backend)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory copy with KernelAbstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have another example to show how to use shared memory in KernelAbstractions. This kernel performs a matrix copy using local memory (also known as shared memory in CUDA), which can significantly speed up the memory access times by reducing global memory bandwidth usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel function lmem_copy_kernel!(output, @Const(input))\n",
    "\n",
    "\t# Gets the global index of the thread in a multidimensional grid, which is used to index into the global input and output arrays.\n",
    "\tI, J= @index(Global, NTuple) \n",
    "\t# Gets the local index within a thread block or workgroup, useful for indexing into locally shared memory.\n",
    "\ti, j = @index(Local, NTuple) # Local index of thread\n",
    "\n",
    "\t#@groupsize() retrieves the dimensions of the thread block or workgroup. \n",
    "\t#The @uniform ensures that these values are treated as constants that are the same for all threads.\n",
    "\tN = @uniform @groupsize()[1] # blockDim.x \n",
    "\tM = @uniform @groupsize()[2] # blockDim.y\n",
    "\n",
    "\n",
    "\ttile = @localmem eltype(output) (N, M) # Allocate local (shared) memory\n",
    "\n",
    "\t#First, data from the global input array is loaded into the shared tile array using local indices.\n",
    "\t@inbounds tile[i, j] = input[I, J]\n",
    "\n",
    "\t#@synchronize ensures that all threads in the workgroup have completed their memory writes to the shared memory before proceeding. \n",
    "\t#This is crucial to prevent race conditions.\n",
    "\t@synchronize\n",
    "\n",
    "\t#Finally, the data is written back from the shared tile array to the global output array.\n",
    "\t@inbounds output[I, J] = tile[i, j]\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "output = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n",
    "\n",
    "const lmem_copy! = lmem_copy_kernel!(Backend, workgroupsize)\n",
    "lmem_copy!(output, input, ndrange=size(input))\n",
    "KernelAbstractions.synchronize(Backend)\n",
    "\n",
    "all(input == output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
