{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://github.com/JuliaLang/julia/raw/master/doc/src/assets/logo.svg)![img](https://avatars.githubusercontent.com/u/7346142?s=200&v=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JuliaGPU packages are easy to install: Just do `Pkg.add(\"CUDA\")` to install the CUDA.jl package, which provides bindings to NVIDIA's CUDA. CUDA.jl provides all of the compiler and runtime logic needed to program NVIDIA GPUs; the only thing you need to provide is a functional NVIDIA driver (which most HPC systems already have installed and configured), but you don't need to install the CUDA toolkit! CUDA.jl downloads one if it's not already available on your system (again, HPC systems usually have this provided for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "# This can take a little while to download and compile, so just be patient\n",
    "import Pkg\n",
    "Pkg.add(\"CUDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be running some small benchmarks in this notebook, so let's also grab Julia's BenchmarkTools.jl while we're at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"BenchmarkTools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we import these packages, along with the built-in LinearAlgebra standard library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, GPU vendor libraries can be difficult, so CUDA.jl provides a convenient way to check if everything is setup correctly, the `CUDA.versioninfo()` function. Like Julia's `Base.versioninfo()`, this will print some information on the available hardware and loaded libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.2, local installation\n",
      "CUDA driver 12.6\n",
      "NVIDIA driver 535.183.6\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.2.1\n",
      "- CURAND: 10.3.3\n",
      "- CUFFT: 11.0.8\n",
      "- CUSOLVER: 11.5.0\n",
      "- CUSPARSE: 12.1.1\n",
      "- CUPTI: 2023.2.0 (API 20.0.0)\n",
      "- NVML: 12.0.0+535.183.6\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.5.2\n",
      "- CUDA_Driver_jll: 0.10.3+0\n",
      "- CUDA_Runtime_jll: 0.15.3+0\n",
      "- CUDA_Runtime_Discovery: 0.3.5\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.10.4\n",
      "- LLVM: 15.0.7\n",
      "\n",
      "Preferences:\n",
      "- CUDA_Runtime_jll.version: 12.2\n",
      "- CUDA_Runtime_jll.local: true\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good practice to check this at least once on a new system or when you mess with `module`s loaded, just to ensure that everything is connected and happy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Vector Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's take a look at vector addition. Let's assume you have two vectors $\\vec{a}$ and $\\vec{b}$ and you want to add them elementwise. You can do this in many ways in Julia: \n",
    "1. simple for loop on a CPU\n",
    "2. julia array add (+) on a CPU or GPU\n",
    "3. GPU kernel programming in CUDA (or KernelAbstractions using CUDA as backend - we'll see this soon!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 4\n",
       " ⋮\n",
       " 4\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our input a, b vectors, and output c vector in CPU RAM\n",
    "vector_size = 1024\n",
    "a = rand(1:4, vector_size)\n",
    "b = rand(1:4, vector_size)\n",
    "c = zeros(Int, vector_size)\n",
    "\n",
    "# what's in a?\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a simple CPU loop to add two vectors in serial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " ⋮\n",
       " 5\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 7\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 6\n",
       " 6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: the exclamation mark (!) doesn't do anything special\n",
    "# It's just used to indicate that a function mutates its arguments\n",
    "function vadd!(a, b, c)\n",
    "    for i in 1:length(c)\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "end\n",
    "vadd!(a, b, c)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, Julia has a ton of built-in array operations, so we don't actually need to implement this ourselves. Julia's vector add (+) operation works exactly as you'd expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Vector{Int64}:\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " ⋮\n",
       " 5\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 7\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 6\n",
       " 6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b\n",
    "\n",
    "# Note that, unlike `vadd!`, the above operation allocates a new `c` as the output vector - this is important to remember."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! But isn't this a GPU tutorial? Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 4\n",
       " ⋮\n",
       " 4\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need first to make copies of the a and b vectors on the GPU, and define a new dc empty GPU vector\n",
    "\n",
    "# The CuArray() function automatically allocates a new GPU array of the same size and shape as the input,\n",
    "# and copies from the input CPU array to the newly-allocated GPU array\n",
    "da = CuArray(a)\n",
    "db = CuArray(b)\n",
    "\n",
    "# CUDA.zeros takes the desired element type and array size, and automatically allocates and initializes\n",
    "# a new GPU array with int64 zeros\n",
    "dc = CUDA.zeros(Int, size(a))\n",
    "\n",
    "# We can also safely take a look at what's in da, even though it's on the GPU!\n",
    "# It's a different array type, and that fact is made clear to us:\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to allocate on the GPU, let's see how to use this same add (+) operation on the GPU to add two of those vectors using CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " ⋮\n",
       " 5\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 7\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 6\n",
       " 6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = da + db\n",
    "\n",
    "# We can add GPU vectors using the same `+` operator, thanks to Julia's multiple dispatch!\n",
    "# Also, like for the CPU add operation, this one also allocates a new GPU array for output `dc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, but vector addition is pretty... simple? Let us learn how to write our own GPU kernels with CUDA.jl in pure Julia.\n",
    "\n",
    "In array operations, CUDA.jl can leverage implicit parallelism (expressed over the array's elements) to automatically execute these operations in parallel on a GPU. When using hand-rolled kernels, it is instead the programmer's responsibility to decide how to effectively assign the available parallel execution resources for the specific operation. Let's see how this is done for vector addition, before moving on to more interesting examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd_kernel! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vadd_kernel!(c, a, b)\n",
    "    # Obtain GPU thread index, which should be mapped to the valid indices of a and b\n",
    "    i = threadIdx().x\n",
    "    # Each thread will add its own element to c\n",
    "    c[i] = a[i] + b[i]\n",
    "    \n",
    "    # GPU kernels don't return anything\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, that's pretty easy, you just need to write a scalar function, just like you'd do if you were writing CUDA C++. Now we just need to launch that function in parallel using the `@cuda` macro, and specify the number of GPU threads with the `threads` keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 5\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " ⋮\n",
       " 5\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 7\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 6\n",
       " 6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch our `vadd_kernel!` GPU kernel, with our GPU arrays as inputs\n",
    "@cuda threads=length(da) vadd_kernel!(dc, da, db)\n",
    "\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this is great, and was not a lot of work for us! But to see a downside of this simple approach, let's try to work with bigger vectors, by setting `vector_size` to 10240:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Number of threads in x-dimension exceeds device limit (10240 > 1024).",
     "output_type": "error",
     "traceback": [
      "Number of threads in x-dimension exceeds device limit (10240 > 1024).",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:35",
      "  [2] diagnose_launch_failure(f::CuFunction, err::CuError; blockdim::CuDim3, threaddim::CuDim3, shmem::Int64)",
      "    @ CUDA /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:97",
      "  [3] launch(::CuFunction, ::CUDA.KernelState, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1}; blocks::Int64, threads::Int64, cooperative::Bool, shmem::Int64, stream::CuStream)",
      "    @ CUDA /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:73",
      "  [4] launch",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:52 [inlined]",
      "  [5] #972",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:189 [inlined]",
      "  [6] macro expansion",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:149 [inlined]",
      "  [7] macro expansion",
      "    @ ./none:0 [inlined]",
      "  [8] convert_arguments",
      "    @ ./none:0 [inlined]",
      "  [9] #cudacall#971",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:191 [inlined]",
      " [10] cudacall",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/lib/cudadrv/execution.jl:187 [inlined]",
      " [11] macro expansion",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/src/compiler/execution.jl:279 [inlined]",
      " [12] macro expansion",
      "    @ ./none:0 [inlined]",
      " [13] #_#1157",
      "    @ ./none:0 [inlined]",
      " [14] top-level scope",
      "    @ /pscratch/sd/t/train921/depot/packages/CUDA/2kjXI/src/compiler/execution.jl:114"
     ]
    }
   ],
   "source": [
    "vector_size = 10240\n",
    "da = CuArray(rand(1:4, vector_size))\n",
    "db = CuArray(rand(1:4, vector_size))\n",
    "dc = CUDA.zeros(Int, vector_size)\n",
    "\n",
    "@cuda threads=length(da) vadd_kernel!(dc, da, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! What is going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs have a limited number of threads they can run on a single streaming multiprocessor (SM) at once, and we just tried to assign too many threads to one SM, which isn't possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To query the number of threads per block, we can inspect CUDA attributes:\n",
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 10240 > 1024, the SM wouldn't have had enough resources to satisfy our request, at least not with the default of a single block per kernel.\n",
    "\n",
    "Thankfully, GPUs also have multiple SMs, so in theory this should be solvable. To take advantage of more than one SM, we need to run a kernel with multiple blocks, as a single block can only execute on one SM (which has limited resources available, as we saw in the query above). In order to exploit multiple blocks, though, we need to understand how to index into our arrays when our index depends not just on our thread index, but also on our block index and block sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA.jl, the expression `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` calculates a unique index for each thread across multiple blocks in a CUDA kernel execution. Here's a breakdown of each component and how they contribute to computing this index:\n",
    "\n",
    "- `threadIdx().x`: This returns the x-coordinate of the thread within its block. It's the thread's index within the block, starting from 1 (unlike C/C++ CUDA where it starts from 0).\n",
    "\n",
    "- `blockIdx().x`: This gives the x-coordinate of the block within the grid. It represents the block's index in the grid, also starting from 1.\n",
    "\n",
    "- `blockDim().x`: This represents the number of threads per block along the x-axis.\n",
    "\n",
    "In essence, the formula `i = threadIdx().x + (blockIdx().x - 1) * blockDim().x` is used to compute a global index for each thread, regardless of how blocks are sized. It positions the threads linearly across all blocks. Here's what each part does:\n",
    "\n",
    "- `(blockIdx().x - 1) * blockDim().x`: This part calculates the offset to the start of the current block. Subtracting 1 from `blockIdx().x` makes it zero-based, and then it is multiplied by the number of threads in each block `(blockDim().x)`. This gives the index of the first thread in the current block relative to the entire grid.\n",
    "\n",
    "- `threadIdx().x`: Adding this to the block offset gives the specific thread's index within the whole grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this, let's now rewrite our kernel to properly handle multiple blocks, using the above global indexing formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vadd_kernel! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vadd_cuda!(c, a, b)\n",
    "    # Calculate a unique index for each thread across multiple blocks\n",
    "    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    \n",
    "    # Ensure that we skip invalid indices, if we over-allocated a few threads\n",
    "    if i <= length(a)\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can launch our kernel with the maximum number of threads per block (1024), and then divide up our computation across multiple 1024-wide blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10240-element CuArray{Int64, 1, CUDA.DeviceMemory}:\n",
       " 5\n",
       " 7\n",
       " 5\n",
       " 4\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 4\n",
       " 6\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 8\n",
       " ⋮\n",
       " 7\n",
       " 2\n",
       " 6\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 7\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda threads=1024 blocks=cld(length(da),1024) vadd_cuda!(dc, da, db) # cld(x, y) is (x / y) with round-up behavior\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Now that we're thoroughly done with vector addition, let's move on to something a bit heavier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Matrix-Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication is a mainstay of all kinds of applications, so we should be able to implement this in Julia with ease. Let's first take a look at doing this on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024×1024 Matrix{Float64}:\n",
       " 0.599324   0.136234   0.210912    …  0.963063    0.0903736   0.46398\n",
       " 0.636538   0.864631   0.00851327     0.232077    0.257339    0.183778\n",
       " 0.591745   0.197747   0.344443       0.196597    0.908298    0.53485\n",
       " 0.921838   0.105807   0.803605       0.459633    0.509019    0.967507\n",
       " 0.342914   0.849087   0.725722       0.884828    0.699581    0.218655\n",
       " 0.100464   0.557632   0.890053    …  0.0495604   0.885375    0.641849\n",
       " 0.304138   0.459314   0.24382        0.326963    0.134166    0.648276\n",
       " 0.33685    0.107411   0.521201       0.291425    0.919088    0.230217\n",
       " 0.453407   0.472649   0.462337       0.0498801   0.873891    0.910634\n",
       " 0.700216   0.0216538  0.279405       0.687379    0.0839788   0.687677\n",
       " 0.717115   0.928108   0.669639    …  0.999369    0.569745    0.241245\n",
       " 0.0106636  0.415395   0.14221        0.153659    0.00537939  0.0914411\n",
       " 0.373983   0.909296   0.174363       0.7676      0.148141    0.0880714\n",
       " ⋮                                 ⋱                          \n",
       " 0.300189   0.826482   0.311731       0.69487     0.427001    0.297775\n",
       " 0.0288446  0.554919   0.986458       0.980198    0.474244    0.906532\n",
       " 0.023617   0.846147   0.0421357      0.835698    0.0164201   0.0699113\n",
       " 0.830035   0.80792    0.0487832   …  0.943737    0.445929    0.296034\n",
       " 0.0116225  0.548254   0.255484       0.00831938  0.684668    0.437825\n",
       " 0.156969   0.0475111  0.246781       0.899187    0.326009    0.731477\n",
       " 0.972905   0.29342    0.527954       0.58589     0.361654    0.616958\n",
       " 0.180286   0.623384   0.272158       0.33207     0.366655    0.977879\n",
       " 0.0691982  0.935355   0.159223    …  0.388523    0.6793      0.735437\n",
       " 0.890177   0.333704   0.833202       0.0401351   0.00876109  0.906011\n",
       " 0.669243   0.937784   0.157047       0.469142    0.894106    0.297702\n",
       " 0.794338   0.624764   0.500493       0.0626692   0.199156    0.729487"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate our random matrix inputs and zero'd output\n",
    "matrix_size = 1024\n",
    "A = rand(matrix_size, matrix_size)\n",
    "B = rand(matrix_size, matrix_size)\n",
    "C = zeros(matrix_size, matrix_size)\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three nested loops implementation of matrix multiplication is easy to express on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixMultiplication! (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function MatrixMultiplication!(C, A, B)\n",
    "    for i in 1:size(C, 1)\n",
    "        for j in 1:size(C, 2)\n",
    "            C[i, j] = 0\n",
    "            for k in 1:size(A, 2)\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 Matrix{Float64}:\n",
       " 522.008  514.447  518.353  508.014  …  503.772  518.87   516.198  510.185\n",
       " 513.823  501.285  509.5    505.66      495.307  511.577  502.189  501.256\n",
       " 537.5    518.566  517.116  523.175     513.203  522.36   520.318  514.112\n",
       " 525.524  510.32   507.626  518.828     505.366  515.603  512.018  511.279\n",
       " 521.791  511.283  505.692  513.941     500.857  511.991  510.209  512.336\n",
       " 509.001  495.895  503.534  504.875  …  498.997  512.214  499.784  502.016\n",
       " 516.151  510.726  503.878  518.626     504.707  515.551  511.767  500.716\n",
       " 521.474  510.29   521.157  521.408     512.018  524.482  513.75   512.201\n",
       " 525.957  512.427  515.678  527.4       507.682  525.113  512.892  508.593\n",
       " 513.135  494.763  502.173  503.78      498.956  503.753  501.352  506.024\n",
       " 526.618  512.589  505.468  517.26   …  496.107  522.325  513.331  501.411\n",
       " 521.852  510.839  498.612  515.724     501.931  519.637  514.263  503.614\n",
       " 516.697  505.221  503.88   508.651     499.331  515.61   514.946  500.09\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 512.96   501.434  500.054  505.385     490.598  513.603  502.559  495.802\n",
       " 530.626  518.186  520.457  527.984     510.104  531.732  522.166  514.673\n",
       " 522.24   513.869  514.765  524.642     511.827  526.358  521.049  517.977\n",
       " 520.732  515.686  504.372  514.285     500.214  522.652  508.912  504.751\n",
       " 529.648  512.648  514.238  515.212  …  503.029  525.328  515.73   504.44\n",
       " 513.226  500.457  507.992  513.741     499.033  517.123  506.46   506.987\n",
       " 512.392  509.757  500.017  516.596     496.211  517.288  497.783  505.848\n",
       " 517.111  501.315  503.653  506.829     496.207  512.616  504.108  502.207\n",
       " 530.955  516.496  520.788  516.419     514.587  529.623  524.514  516.56\n",
       " 527.225  512.622  516.788  519.734  …  514.42   523.694  524.069  513.844\n",
       " 520.191  511.557  510.479  515.198     509.291  523.408  516.387  509.135\n",
       " 521.358  515.678  517.588  515.924     498.943  521.015  506.181  499.834"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MatrixMultiplication!(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, Julia has this one built-in already (it calls BLAS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 Matrix{Float64}:\n",
       " 522.008  514.447  518.353  508.014  …  503.772  518.87   516.198  510.185\n",
       " 513.823  501.285  509.5    505.66      495.307  511.577  502.189  501.256\n",
       " 537.5    518.566  517.116  523.175     513.203  522.36   520.318  514.112\n",
       " 525.524  510.32   507.626  518.828     505.366  515.603  512.018  511.279\n",
       " 521.791  511.283  505.692  513.941     500.857  511.991  510.209  512.336\n",
       " 509.001  495.895  503.534  504.875  …  498.997  512.214  499.784  502.016\n",
       " 516.151  510.726  503.878  518.626     504.707  515.551  511.767  500.716\n",
       " 521.474  510.29   521.157  521.408     512.018  524.482  513.75   512.201\n",
       " 525.957  512.427  515.678  527.4       507.682  525.113  512.892  508.593\n",
       " 513.135  494.763  502.173  503.78      498.956  503.753  501.352  506.024\n",
       " 526.618  512.589  505.468  517.26   …  496.107  522.325  513.331  501.411\n",
       " 521.852  510.839  498.612  515.724     501.931  519.637  514.263  503.614\n",
       " 516.697  505.221  503.88   508.651     499.331  515.61   514.946  500.09\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 512.96   501.434  500.054  505.385     490.598  513.603  502.559  495.802\n",
       " 530.626  518.186  520.457  527.984     510.104  531.732  522.166  514.673\n",
       " 522.24   513.869  514.765  524.642     511.827  526.358  521.049  517.977\n",
       " 520.732  515.686  504.372  514.285     500.214  522.652  508.912  504.751\n",
       " 529.648  512.648  514.238  515.212  …  503.029  525.328  515.73   504.44\n",
       " 513.226  500.457  507.992  513.741     499.033  517.123  506.46   506.987\n",
       " 512.392  509.757  500.017  516.596     496.211  517.288  497.783  505.848\n",
       " 517.111  501.315  503.653  506.829     496.207  512.616  504.108  502.207\n",
       " 530.955  516.496  520.788  516.419     514.587  529.623  524.514  516.56\n",
       " 527.225  512.622  516.788  519.734  …  514.42   523.694  524.069  513.844\n",
       " 520.191  511.557  510.479  515.198     509.291  523.408  516.387  509.135\n",
       " 521.358  515.678  517.588  515.924     498.943  521.015  506.181  499.834"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation performs quite a bit worse than Julia's (OpenBLAS), but that's OK - we haven't really optimized it at all, since this is just a tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 1 sample with 1 evaluation.\n",
       " Single result which took \u001b[34m6.027 s\u001b[39m (0.00% GC) to evaluate,\n",
       " with a memory estimate of \u001b[33m0 bytes\u001b[39m, over \u001b[33m0\u001b[39m allocations."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark MatrixMultiplication!(C, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 197 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 7.025 ms\u001b[22m\u001b[39m … \u001b[35m228.372 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 84.16%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m17.285 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m25.397 ms\u001b[22m\u001b[39m ± \u001b[32m 22.751 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m4.42% ±  7.89%\n",
       "\n",
       "  \u001b[39m█\u001b[39m▃\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[34m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[34m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▄\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▄\u001b[39m▇\u001b[39m█\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▆\u001b[39m \u001b[39m▄\n",
       "  7.02 ms\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      60.2 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m8.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m2\u001b[39m."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's now implement matrix multiplication on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need first to move A and B matrices to the GPU and define a new DC zero'd matrix on the GPU\n",
    "DA = CuArray(A)\n",
    "DB = CuArray(B)\n",
    "DC = CUDA.zeros(size(A))\n",
    "\n",
    "DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, here we can multiply the `DA` matrix by `DB` matrix using the `*` operator (which forwards the call to CUBLAS), thanks again to Julia's multiple dispatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DC = DA * DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function MatrixMultiplication_cuda!(C, A, B)\n",
    "    # Calculate the global row and column indices\n",
    "    row = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    col = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "\n",
    "    # Create a 0 of the same type as C's element type (for type stability)\n",
    "    sum = zero(eltype(C))\n",
    "\n",
    "    if row <= size(A, 1) && col < size(B, 2)\n",
    "        for i in 1:size(A, 2)\n",
    "            # @inbounds disables bounds checking for array accesses, to improve performance\n",
    "            # Note that incorrect usage can result in segfaults/memory faults/wrong results\n",
    "            @inbounds sum += A[row, i] * B[i, col]\n",
    "        end\n",
    "        C[row, col] = sum\n",
    "    end\n",
    "\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048×2048 CuArray{Float64, 2, CUDA.DeviceMemory}:\n",
       " 512.654  491.693  497.835  491.77   …  498.166  497.263  508.779  501.667\n",
       " 506.188  496.73   505.53   499.666     502.857  492.82   505.705  498.718\n",
       " 510.076  494.765  504.817  498.36      509.65   498.694  507.214  508.859\n",
       " 508.611  498.84   506.446  498.362     512.131  503.416  511.78   502.783\n",
       " 516.901  503.984  513.068  506.68      509.245  516.674  517.319  505.767\n",
       " 515.004  500.863  505.083  504.142  …  515.888  503.512  520.505  516.691\n",
       " 516.63   501.365  509.056  507.993     518.387  510.541  517.607  508.491\n",
       " 523.411  508.684  512.396  511.45      518.802  510.688  518.202  512.118\n",
       " 502.255  490.325  496.933  509.615     504.088  494.503  504.638  501.606\n",
       " 527.083  514.173  518.128  511.688     518.804  519.843  521.059  517.776\n",
       " 512.59   496.871  509.555  503.781  …  510.958  502.675  512.275  506.564\n",
       " 522.832  503.34   520.555  513.428     518.154  516.196  519.143  515.926\n",
       " 507.4    493.934  513.169  503.977     506.983  498.082  508.861  504.338\n",
       "   ⋮                                 ⋱             ⋮               \n",
       " 530.649  513.714  526.755  524.751     527.395  518.007  529.764  523.275\n",
       " 512.473  505.039  509.893  501.826     520.384  507.622  523.074  514.969\n",
       " 529.623  521.272  533.074  521.524     526.363  521.678  539.115  532.541\n",
       " 499.122  482.958  500.156  492.002     500.593  488.56   518.84   492.064\n",
       " 523.673  512.821  522.67   517.57   …  525.226  521.007  534.917  516.673\n",
       " 528.994  515.9    525.714  513.199     524.465  518.547  526.604  521.568\n",
       " 539.558  518.265  526.518  525.421     529.443  518.463  532.629  526.191\n",
       " 524.784  501.369  515.311  508.774     520.718  502.321  522.766  509.697\n",
       " 514.196  500.309  507.959  498.45      510.541  505.465  508.122  508.35\n",
       " 527.888  510.261  527.149  515.119  …  525.683  516.986  522.767  524.906\n",
       " 520.275  507.36   515.102  511.58      516.463  512.813  518.11   517.431\n",
       " 514.525  495.576  511.604  509.314     505.446  502.25   512.574  507.629"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split blocks up into 32x32 tiles\n",
    "@cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication_cuda!(DC, DA, DB)\n",
    "\n",
    "DC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we'd expect, the optimized CUBLAS implementation is far faster than ours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 5594 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 20.429 μs\u001b[22m\u001b[39m … \u001b[35m132.732 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 98.82%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m972.307 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m891.426 μs\u001b[22m\u001b[39m ± \u001b[32m  1.789 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.67% ±  1.57%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[34m█\u001b[39m\u001b[39m \u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[34m█\u001b[39m\u001b[39m \u001b[39m▂\n",
       "  20.4 μs\u001b[90m          Histogram: frequency by time\u001b[39m          986 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.17 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m50\u001b[39m."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark DA * DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 545 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m9.129 ms\u001b[22m\u001b[39m … \u001b[35m 9.940 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m9.178 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m9.178 ms\u001b[22m\u001b[39m ± \u001b[32m53.594 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m▃\u001b[39m\u001b[34m▄\u001b[39m\u001b[39m█\u001b[39m▂\u001b[39m▂\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▇\u001b[39m▄\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[32m█\u001b[39m\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▄\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m \u001b[39m▄\n",
       "  9.13 ms\u001b[90m        Histogram: frequency by time\u001b[39m        9.27 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.27 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m36\u001b[39m."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark CUDA.@sync @cuda threads=(32, 32) blocks=(matrix_size ÷ 32, matrix_size ÷ 32) MatrixMultiplication_cuda!(DC, DA, DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch! Why exactly is our kernel implementation slower?\n",
    "\n",
    "The answer is that this is only the naive implementation of matrix multiplication - if you did the same in CUDA C++, you'd get similarly bad performance. The performant implementation relies on tiling, where the matrix is divided into smaller submatrices (tiles) that fit more effectively within the GPU’s memory hierarchy, including shared memory and cache. Additionally, optimized kernels would use shared memory and WMMA instructions to greatly improve data locality and reduce bandwidth needs (both of which can be easily used in Julia).\n",
    "\n",
    "In an optimized implementation, each thread block on the GPU handles a specific tile of the output matrix, loading portions of the input tiles into shared memory to reduce the repeated global memory access. This approach enables a higher level of parallelism by allowing multiple tiles to be processed concurrently across the GPU cores, without bottlenecking on memory transfers.\n",
    "\n",
    "For the purpose of this tutorial, we will not be implementing these optimizations, but do know that they are as easy (or easier) to use in Julia compared to CUDA C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KernelAbstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to write vendor-specific kernels, let's explore the naive matrix multiplication example using KernelAbstractions.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/global/u2/t/train921/julia-hpc-tutorial-sc24-main/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"KernelAbstractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load the Random standard library to assist with certain random array initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using KernelAbstractions\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a kernel with KernelAbstractions is very similar to implementing a kernel with CUDA.jl. The primary differences include annotating a kernel function with `@kernel`, and doing thread indexing using `@index` (which efficiently abstracts away the index calculations we were previously doing). Otherwise, most things are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixMultiplication_kernel! (generic function with 4 methods)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function MatrixMultiplication_kernel!(C, A, B)\n",
    "    # Global index of each thread across multiple blocks in both x and y dimensions of the grid\n",
    "    row, col = @index(Global, NTuple)\n",
    "\n",
    "    # Everything else is the same!\n",
    "    sum = zero(eltype(C))\n",
    "\n",
    "    if row <= size(A, 1) && col <= size(B, 2)\n",
    "        for i = 1:size(A, 2)\n",
    "             @inbounds sum += A[row, i] * B[i, col]\n",
    "        end\n",
    "        @inbounds C[row, col] = sum\n",
    "     end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key difference between KernelAbstractions and CUDA is that, because KernelAbstractions is portable, we need to select the CUDA \"backend\" when we compile our kernel (AMDGPU, Apple, and Intel are also supported). Most operations take the backend as the first argument, to allow Julia's multiple dispatch to redirect calls to the correct implementation. Additionally, KernelAbstractions separate the compilation and kernel launch stages, and provides configurations for each step to optimize further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the CUDA backend\n",
    "Backend = CUDA.CUDABackend()\n",
    "\n",
    "# Use KernelAbstractions's APIs to allocate GPU matrices DA, DB, and DC\n",
    "matrix_size = 2048\n",
    "T = Float64\n",
    "DA = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "DB = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "DC = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n",
    "\n",
    "# Compile the kernel\n",
    "# We'll statically assign the workgroup (AKA block) size to allow for additional compile-time optimizations\n",
    "workgroupsize = (32, 32)\n",
    "kernel! = MatrixMultiplication_kernel!(Backend, workgroupsize)\n",
    "\n",
    "# Launch the kernel with our GPU matrices as inputs\n",
    "kernel!(DC, DA, DB, ndrange=(size(DC)))\n",
    "\n",
    "# Explicitly wait for the kernel to complete\n",
    "KernelAbstractions.synchronize(Backend)\n",
    "\n",
    "# Are our results what we'd expect to see (compared to CUBLAS)?\n",
    "isapprox(DC, DA * DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 348 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m13.950 ms\u001b[22m\u001b[39m … \u001b[35m14.059 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m14.013 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m14.012 ms\u001b[22m\u001b[39m ± \u001b[32m20.283 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▅\u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▁\u001b[39m▂\u001b[39m▆\u001b[39m▅\u001b[39m▃\u001b[34m▄\u001b[39m\u001b[39m▂\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▅\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m \u001b[39m▅\n",
       "  14 ms\u001b[90m           Histogram: frequency by time\u001b[39m        14.1 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.89 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m60\u001b[39m."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark begin\n",
    "    kernel!(DC, DA, DB, ndrange=size(DC))\n",
    "    KernelAbstractions.synchronize(Backend)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Memory copy with KernelAbstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see a different kind of example, to show how to use shared memory in KernelAbstractions. This kernel performs a matrix copy using local memory (also known as shared memory in CUDA), which can significantly speed up the memory access times by reducing global memory bandwidth usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lmem_copy_kernel! (generic function with 4 methods)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function lmem_copy_kernel!(output, @Const(input))\n",
    "    # Gets the global index of the thread in a multidimensional grid, which is used to index into the global input and output arrays.\n",
    "    I, J = @index(Global, NTuple) \n",
    "    # Gets the local index within a thread block or workgroup, useful for indexing into locally shared memory.\n",
    "    i, j = @index(Local, NTuple) # Local index of thread\n",
    "\n",
    "    # @groupsize() retrieves the dimensions of the thread block or workgroup.\n",
    "    # The @uniform ensures that these values are treated as constants that are the same for all threads.\n",
    "    N = @uniform @groupsize()[1] # same as blockDim().x \n",
    "    M = @uniform @groupsize()[2] # same as blockDim().y\n",
    "\n",
    "    # Allocate local (shared) memory\n",
    "    tile = @localmem eltype(output) (N, M)\n",
    "\n",
    "    # First, data from the global input array is loaded into the shared tile array using local indices.\n",
    "    @inbounds tile[i, j] = input[I, J]\n",
    "\n",
    "    # @synchronize ensures that all threads in the workgroup have completed their memory writes to the shared memory before proceeding. \n",
    "    # This is crucial to prevent race conditions.\n",
    "    @synchronize\n",
    "\n",
    "    # Finally, the data is written back from the shared tile array to the global output array.\n",
    "    @inbounds output[I, J] = tile[i, j]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is a little bit more \"advanced\" than prior kernels, but still is quite readable once you know what each of the macros do. We can quickly test that it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate inputs and outputs\n",
    "input = rand!(allocate(Backend, T, matrix_size, matrix_size))\n",
    "output = KernelAbstractions.zeros(Backend, T, matrix_size, matrix_size)\n",
    "\n",
    "# Compile and launch the kernel, and wait for it to complete\n",
    "lmem_copy! = lmem_copy_kernel!(Backend, workgroupsize)\n",
    "lmem_copy!(output, input, ndrange=size(input))\n",
    "KernelAbstractions.synchronize(Backend)\n",
    "\n",
    "# Confirm that the output matrix now matches the input matrix\n",
    "all(input == output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia Tutorial Single Threaded",
   "language": "julia",
   "name": "julia-tutorial-single-threaded"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
